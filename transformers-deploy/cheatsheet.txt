docker run -it --rm --gpus all \
  -v $PWD:/project ghcr.io/els-rd/transformer-deploy:0.5.3 \
  bash -c "cd /project && \
    convert_model -m \"sentence-transformers/LaBSE\" \
    --backend onnx \
    --task embedding \
    --seq-len 16 256 256"


docker run -it --rm --gpus all \
  -v $PWD:/project ghcr.io/els-rd/transformer-deploy:0.5.3 \
  bash -c "cd /project && \
    convert_model -m \"sentence-transformers/multi-qa-mpnet-base-cos-v1\" \
    --backend onnx \
    --task embedding \
    --seq-len 16 128 128"


docker build -t nils_triton nils_docker/

docker run -it --rm --gpus all -p8000:8000 -p8001:8001 -p8002:8002 --shm-size 256m   -v $PWD/triton_models_nils:/models nils_triton   bash -c "tritonserver --model-repository=/models"

#SageMaker limits shm-size to 64MB
docker run -it --rm --gpus all -p8000:8000 -p8001:8001 -p8002:8002 --shm-size 64m   -v $PWD/triton_models_nils:/models nils_triton   bash -c "tritonserver --model-repository=/models"

=> Change in transformer_onnx_model/config.pbtxt output dims: [-1, 768]


docker run -it --rm --gpus all -p8000:8000 -p8001:8001 -p8002:8002 --shm-size 256m \
  -v $PWD/triton_models:/models nvcr.io/nvidia/tritonserver:22.07-py3 \
  bash -c "pip install transformers && tritonserver --model-repository=/models"